{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:24.171025Z",
     "iopub.status.busy": "2024-01-23T21:09:24.170239Z",
     "iopub.status.idle": "2024-01-23T21:09:24.175505Z",
     "shell.execute_reply": "2024-01-23T21:09:24.174541Z",
     "shell.execute_reply.started": "2024-01-23T21:09:24.170994Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:24.177144Z",
     "iopub.status.busy": "2024-01-23T21:09:24.176868Z",
     "iopub.status.idle": "2024-01-23T21:09:24.385903Z",
     "shell.execute_reply": "2024-01-23T21:09:24.385110Z",
     "shell.execute_reply.started": "2024-01-23T21:09:24.177121Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/arab-data/all_data.txt', 'r',encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:24.387771Z",
     "iopub.status.busy": "2024-01-23T21:09:24.387502Z",
     "iopub.status.idle": "2024-01-23T21:09:26.000282Z",
     "shell.execute_reply": "2024-01-23T21:09:25.999296Z",
     "shell.execute_reply.started": "2024-01-23T21:09:24.387748Z"
    }
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:26.001697Z",
     "iopub.status.busy": "2024-01-23T21:09:26.001411Z",
     "iopub.status.idle": "2024-01-23T21:09:26.007290Z",
     "shell.execute_reply": "2024-01-23T21:09:26.006357Z",
     "shell.execute_reply.started": "2024-01-23T21:09:26.001672Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:26.009716Z",
     "iopub.status.busy": "2024-01-23T21:09:26.009425Z",
     "iopub.status.idle": "2024-01-23T21:09:26.018831Z",
     "shell.execute_reply": "2024-01-23T21:09:26.018080Z",
     "shell.execute_reply.started": "2024-01-23T21:09:26.009694Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:26.020686Z",
     "iopub.status.busy": "2024-01-23T21:09:26.019964Z",
     "iopub.status.idle": "2024-01-23T21:09:26.030919Z",
     "shell.execute_reply": "2024-01-23T21:09:26.029946Z",
     "shell.execute_reply.started": "2024-01-23T21:09:26.020654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[61 28 15  6 15 48 15 12 39  6]\n",
      " [25 45  6  2 12  6 54 12 39 15]\n",
      " [32 45 54 45 68 25 12  6 35 45]\n",
      " [13 62 78  6 12 39  0 78 50 68]\n",
      " [35 45 80 45 25 21 45 80 68 12]\n",
      " [80 13 60 19  6 74 45 25 49 12]\n",
      " [43 12 25 78 43 12  1  1 54 15]\n",
      " [68 43 16 13 15 43  6 39 12  6]]\n",
      "\n",
      "y\n",
      " [[28 15  6 15 48 15 12 39  6 35]\n",
      " [45  6  2 12  6 54 12 39 15 19]\n",
      " [45 54 45 68 25 12  6 35 45 80]\n",
      " [62 78  6 12 39  0 78 50 68 25]\n",
      " [45 80 45 25 21 45 80 68 12  6]\n",
      " [13 60 19  6 74 45 25 49 12 59]\n",
      " [12 25 78 43 12  1  1 54 15 70]\n",
      " [43 16 13 15 43  6 39 12  6 15]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:26.032365Z",
     "iopub.status.busy": "2024-01-23T21:09:26.032026Z",
     "iopub.status.idle": "2024-01-23T21:09:26.067293Z",
     "shell.execute_reply": "2024-01-23T21:09:26.066444Z",
     "shell.execute_reply.started": "2024-01-23T21:09:26.032333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:26.068568Z",
     "iopub.status.busy": "2024-01-23T21:09:26.068283Z",
     "iopub.status.idle": "2024-01-23T21:09:26.082207Z",
     "shell.execute_reply": "2024-01-23T21:09:26.081373Z",
     "shell.execute_reply.started": "2024-01-23T21:09:26.068545Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:26.083611Z",
     "iopub.status.busy": "2024-01-23T21:09:26.083341Z",
     "iopub.status.idle": "2024-01-23T21:09:26.101641Z",
     "shell.execute_reply": "2024-01-23T21:09:26.100833Z",
     "shell.execute_reply.started": "2024-01-23T21:09:26.083590Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=5, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = train_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = valid_loss =  criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "                \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        model_name = 'best_loss_so_far.net'\n",
    "\n",
    "        checkpoint = {'n_hidden': net.n_hidden,\n",
    "                       'n_layers': net.n_layers,\n",
    "                       'state_dict': net.state_dict(),\n",
    "                       'tokens': net.chars}\n",
    "\n",
    "        with open(model_name, 'wb') as f:\n",
    "            torch.save(model_name, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:26.103020Z",
     "iopub.status.busy": "2024-01-23T21:09:26.102696Z",
     "iopub.status.idle": "2024-01-23T21:09:26.204469Z",
     "shell.execute_reply": "2024-01-23T21:09:26.203610Z",
     "shell.execute_reply.started": "2024-01-23T21:09:26.102988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(82, 512, num_layers=5, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=82, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden=512\n",
    "n_layers=5\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:09:26.207127Z",
     "iopub.status.busy": "2024-01-23T21:09:26.206862Z",
     "iopub.status.idle": "2024-01-23T21:39:13.960924Z",
     "shell.execute_reply": "2024-01-23T21:39:13.960151Z",
     "shell.execute_reply.started": "2024-01-23T21:09:26.207105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 3.3892... Val Loss: 3.3127\n",
      "Epoch: 1/10... Step: 20... Loss: 3.3567... Val Loss: 3.2767\n",
      "Epoch: 1/10... Step: 30... Loss: 3.3429... Val Loss: 3.2799\n",
      "Epoch: 1/10... Step: 40... Loss: 3.3365... Val Loss: 3.2785\n",
      "Epoch: 1/10... Step: 50... Loss: 3.3218... Val Loss: 3.2701\n",
      "Epoch: 1/10... Step: 60... Loss: 3.3108... Val Loss: 3.2726\n",
      "Epoch: 1/10... Step: 70... Loss: 3.3240... Val Loss: 3.2727\n",
      "Epoch: 1/10... Step: 80... Loss: 3.3202... Val Loss: 3.2741\n",
      "Epoch: 1/10... Step: 90... Loss: 3.3168... Val Loss: 3.2797\n",
      "Epoch: 1/10... Step: 100... Loss: 3.3222... Val Loss: 3.2733\n",
      "Epoch: 1/10... Step: 110... Loss: 3.3245... Val Loss: 3.2799\n",
      "Epoch: 1/10... Step: 120... Loss: 3.3083... Val Loss: 3.2724\n",
      "Epoch: 1/10... Step: 130... Loss: 3.3037... Val Loss: 3.2783\n",
      "Epoch: 1/10... Step: 140... Loss: 3.3027... Val Loss: 3.2739\n",
      "Epoch: 1/10... Step: 150... Loss: 3.2983... Val Loss: 3.2748\n",
      "Epoch: 1/10... Step: 160... Loss: 3.3006... Val Loss: 3.2779\n",
      "Epoch: 1/10... Step: 170... Loss: 3.3137... Val Loss: 3.2776\n",
      "Epoch: 1/10... Step: 180... Loss: 3.2961... Val Loss: 3.2742\n",
      "Epoch: 1/10... Step: 190... Loss: 3.3226... Val Loss: 3.2784\n",
      "Epoch: 1/10... Step: 200... Loss: 3.3061... Val Loss: 3.2782\n",
      "Epoch: 1/10... Step: 210... Loss: 3.3267... Val Loss: 3.2723\n",
      "Epoch: 1/10... Step: 220... Loss: 3.3106... Val Loss: 3.2739\n",
      "Epoch: 1/10... Step: 230... Loss: 3.2949... Val Loss: 3.2790\n",
      "Epoch: 1/10... Step: 240... Loss: 3.3059... Val Loss: 3.2805\n",
      "Epoch: 1/10... Step: 250... Loss: 3.2950... Val Loss: 3.2726\n",
      "Epoch: 1/10... Step: 260... Loss: 3.2870... Val Loss: 3.2811\n",
      "Epoch: 1/10... Step: 270... Loss: 3.2836... Val Loss: 3.2785\n",
      "Epoch: 1/10... Step: 280... Loss: 3.2906... Val Loss: 3.2802\n",
      "Epoch: 1/10... Step: 290... Loss: 3.3038... Val Loss: 3.2751\n",
      "Epoch: 1/10... Step: 300... Loss: 3.3002... Val Loss: 3.2727\n",
      "Epoch: 1/10... Step: 310... Loss: 3.2992... Val Loss: 3.2786\n",
      "Epoch: 1/10... Step: 320... Loss: 3.3091... Val Loss: 3.2792\n",
      "Epoch: 1/10... Step: 330... Loss: 3.3055... Val Loss: 3.2774\n",
      "Epoch: 1/10... Step: 340... Loss: 3.3155... Val Loss: 3.2767\n",
      "Epoch: 1/10... Step: 350... Loss: 3.2927... Val Loss: 3.2799\n",
      "Epoch: 1/10... Step: 360... Loss: 3.3051... Val Loss: 3.2744\n",
      "Epoch: 1/10... Step: 370... Loss: 3.2996... Val Loss: 3.2783\n",
      "Epoch: 1/10... Step: 380... Loss: 3.3003... Val Loss: 3.2767\n",
      "Epoch: 1/10... Step: 390... Loss: 3.2954... Val Loss: 3.2683\n",
      "Epoch: 1/10... Step: 400... Loss: 3.3065... Val Loss: 3.2679\n",
      "Epoch: 1/10... Step: 410... Loss: 3.2970... Val Loss: 3.2759\n",
      "Epoch: 1/10... Step: 420... Loss: 3.3089... Val Loss: 3.2702\n",
      "Epoch: 1/10... Step: 430... Loss: 3.2904... Val Loss: 3.2655\n",
      "Epoch: 1/10... Step: 440... Loss: 3.2944... Val Loss: 3.2745\n",
      "Epoch: 1/10... Step: 450... Loss: 3.2885... Val Loss: 3.2641\n",
      "Epoch: 2/10... Step: 460... Loss: 3.2881... Val Loss: 3.2715\n",
      "Epoch: 2/10... Step: 470... Loss: 3.3046... Val Loss: 3.2690\n",
      "Epoch: 2/10... Step: 480... Loss: 3.3048... Val Loss: 3.2757\n",
      "Epoch: 2/10... Step: 490... Loss: 3.3106... Val Loss: 3.2672\n",
      "Epoch: 2/10... Step: 500... Loss: 3.3043... Val Loss: 3.2685\n",
      "Epoch: 2/10... Step: 510... Loss: 3.2932... Val Loss: 3.2688\n",
      "Epoch: 2/10... Step: 520... Loss: 3.3083... Val Loss: 3.2704\n",
      "Epoch: 2/10... Step: 530... Loss: 3.3066... Val Loss: 3.2717\n",
      "Epoch: 2/10... Step: 540... Loss: 3.3028... Val Loss: 3.2747\n",
      "Epoch: 2/10... Step: 550... Loss: 3.3117... Val Loss: 3.2724\n",
      "Epoch: 2/10... Step: 560... Loss: 3.3135... Val Loss: 3.2777\n",
      "Epoch: 2/10... Step: 570... Loss: 3.3017... Val Loss: 3.2702\n",
      "Epoch: 2/10... Step: 580... Loss: 3.2970... Val Loss: 3.2752\n",
      "Epoch: 2/10... Step: 590... Loss: 3.2967... Val Loss: 3.2722\n",
      "Epoch: 2/10... Step: 600... Loss: 3.2927... Val Loss: 3.2767\n",
      "Epoch: 2/10... Step: 610... Loss: 3.2948... Val Loss: 3.2752\n",
      "Epoch: 2/10... Step: 620... Loss: 3.3090... Val Loss: 3.2742\n",
      "Epoch: 2/10... Step: 630... Loss: 3.2929... Val Loss: 3.2767\n",
      "Epoch: 2/10... Step: 640... Loss: 3.3172... Val Loss: 3.2749\n",
      "Epoch: 2/10... Step: 650... Loss: 3.3048... Val Loss: 3.2773\n",
      "Epoch: 2/10... Step: 660... Loss: 3.3225... Val Loss: 3.2714\n",
      "Epoch: 2/10... Step: 670... Loss: 3.3075... Val Loss: 3.2737\n",
      "Epoch: 2/10... Step: 680... Loss: 3.2922... Val Loss: 3.2770\n",
      "Epoch: 2/10... Step: 690... Loss: 3.3040... Val Loss: 3.2788\n",
      "Epoch: 2/10... Step: 700... Loss: 3.2918... Val Loss: 3.2730\n",
      "Epoch: 2/10... Step: 710... Loss: 3.2868... Val Loss: 3.2808\n",
      "Epoch: 2/10... Step: 720... Loss: 3.2819... Val Loss: 3.2774\n",
      "Epoch: 2/10... Step: 730... Loss: 3.2860... Val Loss: 3.2794\n",
      "Epoch: 2/10... Step: 740... Loss: 3.3024... Val Loss: 3.2735\n",
      "Epoch: 2/10... Step: 750... Loss: 3.2990... Val Loss: 3.2746\n",
      "Epoch: 2/10... Step: 760... Loss: 3.2980... Val Loss: 3.2767\n",
      "Epoch: 2/10... Step: 770... Loss: 3.3069... Val Loss: 3.2789\n",
      "Epoch: 2/10... Step: 780... Loss: 3.3042... Val Loss: 3.2775\n",
      "Epoch: 2/10... Step: 790... Loss: 3.3124... Val Loss: 3.2756\n",
      "Epoch: 2/10... Step: 800... Loss: 3.2898... Val Loss: 3.2810\n",
      "Epoch: 2/10... Step: 810... Loss: 3.3033... Val Loss: 3.2728\n",
      "Epoch: 2/10... Step: 820... Loss: 3.2983... Val Loss: 3.2791\n",
      "Epoch: 2/10... Step: 830... Loss: 3.3003... Val Loss: 3.2754\n",
      "Epoch: 2/10... Step: 840... Loss: 3.2931... Val Loss: 3.2673\n",
      "Epoch: 2/10... Step: 850... Loss: 3.3054... Val Loss: 3.2700\n",
      "Epoch: 2/10... Step: 860... Loss: 3.2959... Val Loss: 3.2744\n",
      "Epoch: 2/10... Step: 870... Loss: 3.3079... Val Loss: 3.2690\n",
      "Epoch: 2/10... Step: 880... Loss: 3.2896... Val Loss: 3.2665\n",
      "Epoch: 2/10... Step: 890... Loss: 3.2928... Val Loss: 3.2740\n",
      "Epoch: 2/10... Step: 900... Loss: 3.2886... Val Loss: 3.2632\n",
      "Epoch: 3/10... Step: 910... Loss: 3.2882... Val Loss: 3.2732\n",
      "Epoch: 3/10... Step: 920... Loss: 3.3036... Val Loss: 3.2669\n",
      "Epoch: 3/10... Step: 930... Loss: 3.3028... Val Loss: 3.2754\n",
      "Epoch: 3/10... Step: 940... Loss: 3.3093... Val Loss: 3.2673\n",
      "Epoch: 3/10... Step: 950... Loss: 3.3026... Val Loss: 3.2679\n",
      "Epoch: 3/10... Step: 960... Loss: 3.2936... Val Loss: 3.2694\n",
      "Epoch: 3/10... Step: 970... Loss: 3.3085... Val Loss: 3.2693\n",
      "Epoch: 3/10... Step: 980... Loss: 3.3061... Val Loss: 3.2724\n",
      "Epoch: 3/10... Step: 990... Loss: 3.3013... Val Loss: 3.2726\n",
      "Epoch: 3/10... Step: 1000... Loss: 3.3091... Val Loss: 3.2738\n",
      "Epoch: 3/10... Step: 1010... Loss: 3.3122... Val Loss: 3.2768\n",
      "Epoch: 3/10... Step: 1020... Loss: 3.3024... Val Loss: 3.2704\n",
      "Epoch: 3/10... Step: 1030... Loss: 3.2954... Val Loss: 3.2744\n",
      "Epoch: 3/10... Step: 1040... Loss: 3.2973... Val Loss: 3.2728\n",
      "Epoch: 3/10... Step: 1050... Loss: 3.2922... Val Loss: 3.2765\n",
      "Epoch: 3/10... Step: 1060... Loss: 3.2938... Val Loss: 3.2745\n",
      "Epoch: 3/10... Step: 1070... Loss: 3.3067... Val Loss: 3.2738\n",
      "Epoch: 3/10... Step: 1080... Loss: 3.2923... Val Loss: 3.2778\n",
      "Epoch: 3/10... Step: 1090... Loss: 3.3172... Val Loss: 3.2737\n",
      "Epoch: 3/10... Step: 1100... Loss: 3.3039... Val Loss: 3.2778\n",
      "Epoch: 3/10... Step: 1110... Loss: 3.3216... Val Loss: 3.2720\n",
      "Epoch: 3/10... Step: 1120... Loss: 3.3072... Val Loss: 3.2733\n",
      "Epoch: 3/10... Step: 1130... Loss: 3.2910... Val Loss: 3.2770\n",
      "Epoch: 3/10... Step: 1140... Loss: 3.3028... Val Loss: 3.2787\n",
      "Epoch: 3/10... Step: 1150... Loss: 3.2904... Val Loss: 3.2725\n",
      "Epoch: 3/10... Step: 1160... Loss: 3.2850... Val Loss: 3.2810\n",
      "Epoch: 3/10... Step: 1170... Loss: 3.2807... Val Loss: 3.2773\n",
      "Epoch: 3/10... Step: 1180... Loss: 3.2863... Val Loss: 3.2792\n",
      "Epoch: 3/10... Step: 1190... Loss: 3.3019... Val Loss: 3.2724\n",
      "Epoch: 3/10... Step: 1200... Loss: 3.2970... Val Loss: 3.2759\n",
      "Epoch: 3/10... Step: 1210... Loss: 3.2970... Val Loss: 3.2752\n",
      "Epoch: 3/10... Step: 1220... Loss: 3.3063... Val Loss: 3.2792\n",
      "Epoch: 3/10... Step: 1230... Loss: 3.3028... Val Loss: 3.2773\n",
      "Epoch: 3/10... Step: 1240... Loss: 3.3123... Val Loss: 3.2754\n",
      "Epoch: 3/10... Step: 1250... Loss: 3.2903... Val Loss: 3.2805\n",
      "Epoch: 3/10... Step: 1260... Loss: 3.3024... Val Loss: 3.2726\n",
      "Epoch: 3/10... Step: 1270... Loss: 3.2973... Val Loss: 3.2789\n",
      "Epoch: 3/10... Step: 1280... Loss: 3.2985... Val Loss: 3.2753\n",
      "Epoch: 3/10... Step: 1290... Loss: 3.2924... Val Loss: 3.2667\n",
      "Epoch: 3/10... Step: 1300... Loss: 3.3057... Val Loss: 3.2710\n",
      "Epoch: 3/10... Step: 1310... Loss: 3.2950... Val Loss: 3.2731\n",
      "Epoch: 3/10... Step: 1320... Loss: 3.3070... Val Loss: 3.2691\n",
      "Epoch: 3/10... Step: 1330... Loss: 3.2883... Val Loss: 3.2664\n",
      "Epoch: 3/10... Step: 1340... Loss: 3.2924... Val Loss: 3.2727\n",
      "Epoch: 3/10... Step: 1350... Loss: 3.2872... Val Loss: 3.2635\n",
      "Epoch: 4/10... Step: 1360... Loss: 3.2863... Val Loss: 3.2740\n",
      "Epoch: 4/10... Step: 1370... Loss: 3.3032... Val Loss: 3.2657\n",
      "Epoch: 4/10... Step: 1380... Loss: 3.3035... Val Loss: 3.2760\n",
      "Epoch: 4/10... Step: 1390... Loss: 3.3095... Val Loss: 3.2670\n",
      "Epoch: 4/10... Step: 1400... Loss: 3.3023... Val Loss: 3.2674\n",
      "Epoch: 4/10... Step: 1410... Loss: 3.2935... Val Loss: 3.2698\n",
      "Epoch: 4/10... Step: 1420... Loss: 3.3072... Val Loss: 3.2686\n",
      "Epoch: 4/10... Step: 1430... Loss: 3.3061... Val Loss: 3.2729\n",
      "Epoch: 4/10... Step: 1440... Loss: 3.3010... Val Loss: 3.2717\n",
      "Epoch: 4/10... Step: 1450... Loss: 3.3100... Val Loss: 3.2746\n",
      "Epoch: 4/10... Step: 1460... Loss: 3.3118... Val Loss: 3.2760\n",
      "Epoch: 4/10... Step: 1470... Loss: 3.3014... Val Loss: 3.2711\n",
      "Epoch: 4/10... Step: 1480... Loss: 3.2967... Val Loss: 3.2734\n",
      "Epoch: 4/10... Step: 1490... Loss: 3.2964... Val Loss: 3.2734\n",
      "Epoch: 4/10... Step: 1500... Loss: 3.2914... Val Loss: 3.2760\n",
      "Epoch: 4/10... Step: 1510... Loss: 3.2937... Val Loss: 3.2744\n",
      "Epoch: 4/10... Step: 1520... Loss: 3.3071... Val Loss: 3.2735\n",
      "Epoch: 4/10... Step: 1530... Loss: 3.2917... Val Loss: 3.2781\n",
      "Epoch: 4/10... Step: 1540... Loss: 3.3162... Val Loss: 3.2732\n",
      "Epoch: 4/10... Step: 1550... Loss: 3.3031... Val Loss: 3.2777\n",
      "Epoch: 4/10... Step: 1560... Loss: 3.3219... Val Loss: 3.2723\n",
      "Epoch: 4/10... Step: 1570... Loss: 3.3058... Val Loss: 3.2728\n",
      "Epoch: 4/10... Step: 1580... Loss: 3.2906... Val Loss: 3.2769\n",
      "Epoch: 4/10... Step: 1590... Loss: 3.3022... Val Loss: 3.2783\n",
      "Epoch: 4/10... Step: 1600... Loss: 3.2897... Val Loss: 3.2735\n",
      "Epoch: 4/10... Step: 1610... Loss: 3.2846... Val Loss: 3.2802\n",
      "Epoch: 4/10... Step: 1620... Loss: 3.2805... Val Loss: 3.2774\n",
      "Epoch: 4/10... Step: 1630... Loss: 3.2859... Val Loss: 3.2795\n",
      "Epoch: 4/10... Step: 1640... Loss: 3.3009... Val Loss: 3.2718\n",
      "Epoch: 4/10... Step: 1650... Loss: 3.2970... Val Loss: 3.2770\n",
      "Epoch: 4/10... Step: 1660... Loss: 3.2979... Val Loss: 3.2737\n",
      "Epoch: 4/10... Step: 1670... Loss: 3.3066... Val Loss: 3.2799\n",
      "Epoch: 4/10... Step: 1680... Loss: 3.3031... Val Loss: 3.2773\n",
      "Epoch: 4/10... Step: 1690... Loss: 3.3117... Val Loss: 3.2753\n",
      "Epoch: 4/10... Step: 1700... Loss: 3.2896... Val Loss: 3.2803\n",
      "Epoch: 4/10... Step: 1710... Loss: 3.3027... Val Loss: 3.2726\n",
      "Epoch: 4/10... Step: 1720... Loss: 3.2976... Val Loss: 3.2786\n",
      "Epoch: 4/10... Step: 1730... Loss: 3.2986... Val Loss: 3.2756\n",
      "Epoch: 4/10... Step: 1740... Loss: 3.2928... Val Loss: 3.2664\n",
      "Epoch: 4/10... Step: 1750... Loss: 3.3050... Val Loss: 3.2710\n",
      "Epoch: 4/10... Step: 1760... Loss: 3.2955... Val Loss: 3.2725\n",
      "Epoch: 4/10... Step: 1770... Loss: 3.3060... Val Loss: 3.2691\n",
      "Epoch: 4/10... Step: 1780... Loss: 3.2878... Val Loss: 3.2670\n",
      "Epoch: 4/10... Step: 1790... Loss: 3.2918... Val Loss: 3.2713\n",
      "Epoch: 4/10... Step: 1800... Loss: 3.2871... Val Loss: 3.2642\n",
      "Epoch: 5/10... Step: 1810... Loss: 3.2866... Val Loss: 3.2740\n",
      "Epoch: 5/10... Step: 1820... Loss: 3.3028... Val Loss: 3.2653\n",
      "Epoch: 5/10... Step: 1830... Loss: 3.3024... Val Loss: 3.2758\n",
      "Epoch: 5/10... Step: 1840... Loss: 3.3092... Val Loss: 3.2671\n",
      "Epoch: 5/10... Step: 1850... Loss: 3.3022... Val Loss: 3.2676\n",
      "Epoch: 5/10... Step: 1860... Loss: 3.2934... Val Loss: 3.2694\n",
      "Epoch: 5/10... Step: 1870... Loss: 3.3078... Val Loss: 3.2686\n",
      "Epoch: 5/10... Step: 1880... Loss: 3.3062... Val Loss: 3.2728\n",
      "Epoch: 5/10... Step: 1890... Loss: 3.3008... Val Loss: 3.2711\n",
      "Epoch: 5/10... Step: 1900... Loss: 3.3095... Val Loss: 3.2751\n",
      "Epoch: 5/10... Step: 1910... Loss: 3.3124... Val Loss: 3.2754\n",
      "Epoch: 5/10... Step: 1920... Loss: 3.3012... Val Loss: 3.2717\n",
      "Epoch: 5/10... Step: 1930... Loss: 3.2959... Val Loss: 3.2726\n",
      "Epoch: 5/10... Step: 1940... Loss: 3.2954... Val Loss: 3.2739\n",
      "Epoch: 5/10... Step: 1950... Loss: 3.2915... Val Loss: 3.2760\n",
      "Epoch: 5/10... Step: 1960... Loss: 3.2934... Val Loss: 3.2742\n",
      "Epoch: 5/10... Step: 1970... Loss: 3.3072... Val Loss: 3.2733\n",
      "Epoch: 5/10... Step: 1980... Loss: 3.2923... Val Loss: 3.2781\n",
      "Epoch: 5/10... Step: 1990... Loss: 3.3160... Val Loss: 3.2730\n",
      "Epoch: 5/10... Step: 2000... Loss: 3.3027... Val Loss: 3.2775\n",
      "Epoch: 5/10... Step: 2010... Loss: 3.3204... Val Loss: 3.2729\n",
      "Epoch: 5/10... Step: 2020... Loss: 3.3060... Val Loss: 3.2718\n",
      "Epoch: 5/10... Step: 2030... Loss: 3.2906... Val Loss: 3.2774\n",
      "Epoch: 5/10... Step: 2040... Loss: 3.3021... Val Loss: 3.2775\n",
      "Epoch: 5/10... Step: 2050... Loss: 3.2902... Val Loss: 3.2743\n",
      "Epoch: 5/10... Step: 2060... Loss: 3.2834... Val Loss: 3.2795\n",
      "Epoch: 5/10... Step: 2070... Loss: 3.2806... Val Loss: 3.2779\n",
      "Epoch: 5/10... Step: 2080... Loss: 3.2848... Val Loss: 3.2788\n",
      "Epoch: 5/10... Step: 2090... Loss: 3.3015... Val Loss: 3.2721\n",
      "Epoch: 5/10... Step: 2100... Loss: 3.2967... Val Loss: 3.2768\n",
      "Epoch: 5/10... Step: 2110... Loss: 3.2969... Val Loss: 3.2733\n",
      "Epoch: 5/10... Step: 2120... Loss: 3.3069... Val Loss: 3.2798\n",
      "Epoch: 5/10... Step: 2130... Loss: 3.3025... Val Loss: 3.2773\n",
      "Epoch: 5/10... Step: 2140... Loss: 3.3115... Val Loss: 3.2752\n",
      "Epoch: 5/10... Step: 2150... Loss: 3.2891... Val Loss: 3.2801\n",
      "Epoch: 5/10... Step: 2160... Loss: 3.3030... Val Loss: 3.2734\n",
      "Epoch: 5/10... Step: 2170... Loss: 3.2968... Val Loss: 3.2779\n",
      "Epoch: 5/10... Step: 2180... Loss: 3.2981... Val Loss: 3.2763\n",
      "Epoch: 5/10... Step: 2190... Loss: 3.2923... Val Loss: 3.2660\n",
      "Epoch: 5/10... Step: 2200... Loss: 3.3052... Val Loss: 3.2712\n",
      "Epoch: 5/10... Step: 2210... Loss: 3.2955... Val Loss: 3.2724\n",
      "Epoch: 5/10... Step: 2220... Loss: 3.3061... Val Loss: 3.2690\n",
      "Epoch: 5/10... Step: 2230... Loss: 3.2890... Val Loss: 3.2671\n",
      "Epoch: 5/10... Step: 2240... Loss: 3.2916... Val Loss: 3.2706\n",
      "Epoch: 5/10... Step: 2250... Loss: 3.2870... Val Loss: 3.2648\n",
      "Epoch: 6/10... Step: 2260... Loss: 3.2857... Val Loss: 3.2736\n",
      "Epoch: 6/10... Step: 2270... Loss: 3.3028... Val Loss: 3.2654\n",
      "Epoch: 6/10... Step: 2280... Loss: 3.3027... Val Loss: 3.2754\n",
      "Epoch: 6/10... Step: 2290... Loss: 3.3092... Val Loss: 3.2671\n",
      "Epoch: 6/10... Step: 2300... Loss: 3.3030... Val Loss: 3.2676\n",
      "Epoch: 6/10... Step: 2310... Loss: 3.2931... Val Loss: 3.2691\n",
      "Epoch: 6/10... Step: 2320... Loss: 3.3067... Val Loss: 3.2686\n",
      "Epoch: 6/10... Step: 2330... Loss: 3.3059... Val Loss: 3.2723\n",
      "Epoch: 6/10... Step: 2340... Loss: 3.3010... Val Loss: 3.2710\n",
      "Epoch: 6/10... Step: 2350... Loss: 3.3082... Val Loss: 3.2750\n",
      "Epoch: 6/10... Step: 2360... Loss: 3.3121... Val Loss: 3.2754\n",
      "Epoch: 6/10... Step: 2370... Loss: 3.3008... Val Loss: 3.2719\n",
      "Epoch: 6/10... Step: 2380... Loss: 3.2960... Val Loss: 3.2723\n",
      "Epoch: 6/10... Step: 2390... Loss: 3.2956... Val Loss: 3.2741\n",
      "Epoch: 6/10... Step: 2400... Loss: 3.2920... Val Loss: 3.2758\n",
      "Epoch: 6/10... Step: 2410... Loss: 3.2936... Val Loss: 3.2744\n",
      "Epoch: 6/10... Step: 2420... Loss: 3.3070... Val Loss: 3.2729\n",
      "Epoch: 6/10... Step: 2430... Loss: 3.2919... Val Loss: 3.2782\n",
      "Epoch: 6/10... Step: 2440... Loss: 3.3157... Val Loss: 3.2727\n",
      "Epoch: 6/10... Step: 2450... Loss: 3.3025... Val Loss: 3.2776\n",
      "Epoch: 6/10... Step: 2460... Loss: 3.3211... Val Loss: 3.2727\n",
      "Epoch: 6/10... Step: 2470... Loss: 3.3053... Val Loss: 3.2719\n",
      "Epoch: 6/10... Step: 2480... Loss: 3.2907... Val Loss: 3.2773\n",
      "Epoch: 6/10... Step: 2490... Loss: 3.3020... Val Loss: 3.2774\n",
      "Epoch: 6/10... Step: 2500... Loss: 3.2902... Val Loss: 3.2746\n",
      "Epoch: 6/10... Step: 2510... Loss: 3.2837... Val Loss: 3.2789\n",
      "Epoch: 6/10... Step: 2520... Loss: 3.2806... Val Loss: 3.2781\n",
      "Epoch: 6/10... Step: 2530... Loss: 3.2852... Val Loss: 3.2784\n",
      "Epoch: 6/10... Step: 2540... Loss: 3.3013... Val Loss: 3.2721\n",
      "Epoch: 6/10... Step: 2550... Loss: 3.2972... Val Loss: 3.2773\n",
      "Epoch: 6/10... Step: 2560... Loss: 3.2961... Val Loss: 3.2723\n",
      "Epoch: 6/10... Step: 2570... Loss: 3.3059... Val Loss: 3.2800\n",
      "Epoch: 6/10... Step: 2580... Loss: 3.3029... Val Loss: 3.2774\n",
      "Epoch: 6/10... Step: 2590... Loss: 3.3120... Val Loss: 3.2751\n",
      "Epoch: 6/10... Step: 2600... Loss: 3.2893... Val Loss: 3.2798\n",
      "Epoch: 6/10... Step: 2610... Loss: 3.3029... Val Loss: 3.2740\n",
      "Epoch: 6/10... Step: 2620... Loss: 3.2976... Val Loss: 3.2772\n",
      "Epoch: 6/10... Step: 2630... Loss: 3.2982... Val Loss: 3.2767\n",
      "Epoch: 6/10... Step: 2640... Loss: 3.2928... Val Loss: 3.2659\n",
      "Epoch: 6/10... Step: 2650... Loss: 3.3041... Val Loss: 3.2708\n",
      "Epoch: 6/10... Step: 2660... Loss: 3.2950... Val Loss: 3.2724\n",
      "Epoch: 6/10... Step: 2670... Loss: 3.3075... Val Loss: 3.2688\n",
      "Epoch: 6/10... Step: 2680... Loss: 3.2887... Val Loss: 3.2675\n",
      "Epoch: 6/10... Step: 2690... Loss: 3.2914... Val Loss: 3.2697\n",
      "Epoch: 6/10... Step: 2700... Loss: 3.2861... Val Loss: 3.2651\n",
      "Epoch: 7/10... Step: 2710... Loss: 3.2857... Val Loss: 3.2733\n",
      "Epoch: 7/10... Step: 2720... Loss: 3.3028... Val Loss: 3.2658\n",
      "Epoch: 7/10... Step: 2730... Loss: 3.3025... Val Loss: 3.2748\n",
      "Epoch: 7/10... Step: 2740... Loss: 3.3079... Val Loss: 3.2679\n",
      "Epoch: 7/10... Step: 2750... Loss: 3.3023... Val Loss: 3.2669\n",
      "Epoch: 7/10... Step: 2760... Loss: 3.2927... Val Loss: 3.2692\n",
      "Epoch: 7/10... Step: 2770... Loss: 3.3064... Val Loss: 3.2686\n",
      "Epoch: 7/10... Step: 2780... Loss: 3.3061... Val Loss: 3.2722\n",
      "Epoch: 7/10... Step: 2790... Loss: 3.3002... Val Loss: 3.2711\n",
      "Epoch: 7/10... Step: 2800... Loss: 3.3094... Val Loss: 3.2750\n",
      "Epoch: 7/10... Step: 2810... Loss: 3.3116... Val Loss: 3.2753\n",
      "Epoch: 7/10... Step: 2820... Loss: 3.3009... Val Loss: 3.2720\n",
      "Epoch: 7/10... Step: 2830... Loss: 3.2951... Val Loss: 3.2719\n",
      "Epoch: 7/10... Step: 2840... Loss: 3.2955... Val Loss: 3.2742\n",
      "Epoch: 7/10... Step: 2850... Loss: 3.2916... Val Loss: 3.2759\n",
      "Epoch: 7/10... Step: 2860... Loss: 3.2935... Val Loss: 3.2745\n",
      "Epoch: 7/10... Step: 2870... Loss: 3.3068... Val Loss: 3.2726\n",
      "Epoch: 7/10... Step: 2880... Loss: 3.2908... Val Loss: 3.2779\n",
      "Epoch: 7/10... Step: 2890... Loss: 3.3153... Val Loss: 3.2731\n",
      "Epoch: 7/10... Step: 2900... Loss: 3.3018... Val Loss: 3.2770\n",
      "Epoch: 7/10... Step: 2910... Loss: 3.3204... Val Loss: 3.2732\n",
      "Epoch: 7/10... Step: 2920... Loss: 3.3071... Val Loss: 3.2714\n",
      "Epoch: 7/10... Step: 2930... Loss: 3.2902... Val Loss: 3.2780\n",
      "Epoch: 7/10... Step: 2940... Loss: 3.3022... Val Loss: 3.2767\n",
      "Epoch: 7/10... Step: 2950... Loss: 3.2897... Val Loss: 3.2754\n",
      "Epoch: 7/10... Step: 2960... Loss: 3.2837... Val Loss: 3.2779\n",
      "Epoch: 7/10... Step: 2970... Loss: 3.2798... Val Loss: 3.2788\n",
      "Epoch: 7/10... Step: 2980... Loss: 3.2853... Val Loss: 3.2780\n",
      "Epoch: 7/10... Step: 2990... Loss: 3.3006... Val Loss: 3.2722\n",
      "Epoch: 7/10... Step: 3000... Loss: 3.2973... Val Loss: 3.2774\n",
      "Epoch: 7/10... Step: 3010... Loss: 3.2968... Val Loss: 3.2720\n",
      "Epoch: 7/10... Step: 3020... Loss: 3.3060... Val Loss: 3.2800\n",
      "Epoch: 7/10... Step: 3030... Loss: 3.3018... Val Loss: 3.2774\n",
      "Epoch: 7/10... Step: 3040... Loss: 3.3117... Val Loss: 3.2751\n",
      "Epoch: 7/10... Step: 3050... Loss: 3.2893... Val Loss: 3.2799\n",
      "Epoch: 7/10... Step: 3060... Loss: 3.3025... Val Loss: 3.2743\n",
      "Epoch: 7/10... Step: 3070... Loss: 3.2972... Val Loss: 3.2766\n",
      "Epoch: 7/10... Step: 3080... Loss: 3.2978... Val Loss: 3.2772\n",
      "Epoch: 7/10... Step: 3090... Loss: 3.2923... Val Loss: 3.2656\n",
      "Epoch: 7/10... Step: 3100... Loss: 3.3038... Val Loss: 3.2709\n",
      "Epoch: 7/10... Step: 3110... Loss: 3.2948... Val Loss: 3.2723\n",
      "Epoch: 7/10... Step: 3120... Loss: 3.3070... Val Loss: 3.2689\n",
      "Epoch: 7/10... Step: 3130... Loss: 3.2883... Val Loss: 3.2675\n",
      "Epoch: 7/10... Step: 3140... Loss: 3.2910... Val Loss: 3.2693\n",
      "Epoch: 7/10... Step: 3150... Loss: 3.2865... Val Loss: 3.2655\n",
      "Epoch: 8/10... Step: 3160... Loss: 3.2848... Val Loss: 3.2727\n",
      "Epoch: 8/10... Step: 3170... Loss: 3.3025... Val Loss: 3.2664\n",
      "Epoch: 8/10... Step: 3180... Loss: 3.3024... Val Loss: 3.2738\n",
      "Epoch: 8/10... Step: 3190... Loss: 3.3083... Val Loss: 3.2682\n",
      "Epoch: 8/10... Step: 3200... Loss: 3.3026... Val Loss: 3.2668\n",
      "Epoch: 8/10... Step: 3210... Loss: 3.2925... Val Loss: 3.2690\n",
      "Epoch: 8/10... Step: 3220... Loss: 3.3063... Val Loss: 3.2686\n",
      "Epoch: 8/10... Step: 3230... Loss: 3.3065... Val Loss: 3.2720\n",
      "Epoch: 8/10... Step: 3240... Loss: 3.3009... Val Loss: 3.2710\n",
      "Epoch: 8/10... Step: 3250... Loss: 3.3091... Val Loss: 3.2748\n",
      "Epoch: 8/10... Step: 3260... Loss: 3.3122... Val Loss: 3.2754\n",
      "Epoch: 8/10... Step: 3270... Loss: 3.3008... Val Loss: 3.2723\n",
      "Epoch: 8/10... Step: 3280... Loss: 3.2958... Val Loss: 3.2716\n",
      "Epoch: 8/10... Step: 3290... Loss: 3.2947... Val Loss: 3.2745\n",
      "Epoch: 8/10... Step: 3300... Loss: 3.2917... Val Loss: 3.2756\n",
      "Epoch: 8/10... Step: 3310... Loss: 3.2929... Val Loss: 3.2746\n",
      "Epoch: 8/10... Step: 3320... Loss: 3.3070... Val Loss: 3.2727\n",
      "Epoch: 8/10... Step: 3330... Loss: 3.2906... Val Loss: 3.2779\n",
      "Epoch: 8/10... Step: 3340... Loss: 3.3155... Val Loss: 3.2732\n",
      "Epoch: 8/10... Step: 3350... Loss: 3.3024... Val Loss: 3.2768\n",
      "Epoch: 8/10... Step: 3360... Loss: 3.3212... Val Loss: 3.2735\n",
      "Epoch: 8/10... Step: 3370... Loss: 3.3066... Val Loss: 3.2709\n",
      "Epoch: 8/10... Step: 3380... Loss: 3.2900... Val Loss: 3.2778\n",
      "Epoch: 8/10... Step: 3390... Loss: 3.3022... Val Loss: 3.2767\n",
      "Epoch: 8/10... Step: 3400... Loss: 3.2894... Val Loss: 3.2757\n",
      "Epoch: 8/10... Step: 3410... Loss: 3.2834... Val Loss: 3.2775\n",
      "Epoch: 8/10... Step: 3420... Loss: 3.2794... Val Loss: 3.2793\n",
      "Epoch: 8/10... Step: 3430... Loss: 3.2852... Val Loss: 3.2777\n",
      "Epoch: 8/10... Step: 3440... Loss: 3.3006... Val Loss: 3.2719\n",
      "Epoch: 8/10... Step: 3450... Loss: 3.2961... Val Loss: 3.2777\n",
      "Epoch: 8/10... Step: 3460... Loss: 3.2960... Val Loss: 3.2719\n",
      "Epoch: 8/10... Step: 3470... Loss: 3.3049... Val Loss: 3.2796\n",
      "Epoch: 8/10... Step: 3480... Loss: 3.3025... Val Loss: 3.2777\n",
      "Epoch: 8/10... Step: 3490... Loss: 3.3114... Val Loss: 3.2752\n",
      "Epoch: 8/10... Step: 3500... Loss: 3.2886... Val Loss: 3.2794\n",
      "Epoch: 8/10... Step: 3510... Loss: 3.3023... Val Loss: 3.2747\n",
      "Epoch: 8/10... Step: 3520... Loss: 3.2975... Val Loss: 3.2761\n",
      "Epoch: 8/10... Step: 3530... Loss: 3.2972... Val Loss: 3.2775\n",
      "Epoch: 8/10... Step: 3540... Loss: 3.2921... Val Loss: 3.2657\n",
      "Epoch: 8/10... Step: 3550... Loss: 3.3034... Val Loss: 3.2705\n",
      "Epoch: 8/10... Step: 3560... Loss: 3.2950... Val Loss: 3.2723\n",
      "Epoch: 8/10... Step: 3570... Loss: 3.3061... Val Loss: 3.2690\n",
      "Epoch: 8/10... Step: 3580... Loss: 3.2881... Val Loss: 3.2677\n",
      "Epoch: 8/10... Step: 3590... Loss: 3.2910... Val Loss: 3.2685\n",
      "Epoch: 8/10... Step: 3600... Loss: 3.2855... Val Loss: 3.2657\n",
      "Epoch: 9/10... Step: 3610... Loss: 3.2850... Val Loss: 3.2724\n",
      "Epoch: 9/10... Step: 3620... Loss: 3.3028... Val Loss: 3.2667\n",
      "Epoch: 9/10... Step: 3630... Loss: 3.3024... Val Loss: 3.2731\n",
      "Epoch: 9/10... Step: 3640... Loss: 3.3077... Val Loss: 3.2690\n",
      "Epoch: 9/10... Step: 3650... Loss: 3.3025... Val Loss: 3.2665\n",
      "Epoch: 9/10... Step: 3660... Loss: 3.2926... Val Loss: 3.2690\n",
      "Epoch: 9/10... Step: 3670... Loss: 3.3061... Val Loss: 3.2686\n",
      "Epoch: 9/10... Step: 3680... Loss: 3.3054... Val Loss: 3.2717\n",
      "Epoch: 9/10... Step: 3690... Loss: 3.2999... Val Loss: 3.2712\n",
      "Epoch: 9/10... Step: 3700... Loss: 3.3088... Val Loss: 3.2744\n",
      "Epoch: 9/10... Step: 3710... Loss: 3.3121... Val Loss: 3.2756\n",
      "Epoch: 9/10... Step: 3720... Loss: 3.3001... Val Loss: 3.2724\n",
      "Epoch: 9/10... Step: 3730... Loss: 3.2960... Val Loss: 3.2714\n",
      "Epoch: 9/10... Step: 3740... Loss: 3.2958... Val Loss: 3.2747\n",
      "Epoch: 9/10... Step: 3750... Loss: 3.2912... Val Loss: 3.2755\n",
      "Epoch: 9/10... Step: 3760... Loss: 3.2927... Val Loss: 3.2751\n",
      "Epoch: 9/10... Step: 3770... Loss: 3.3061... Val Loss: 3.2721\n",
      "Epoch: 9/10... Step: 3780... Loss: 3.2907... Val Loss: 3.2778\n",
      "Epoch: 9/10... Step: 3790... Loss: 3.3160... Val Loss: 3.2736\n",
      "Epoch: 9/10... Step: 3800... Loss: 3.3030... Val Loss: 3.2763\n",
      "Epoch: 9/10... Step: 3810... Loss: 3.3205... Val Loss: 3.2738\n",
      "Epoch: 9/10... Step: 3820... Loss: 3.3076... Val Loss: 3.2706\n",
      "Epoch: 9/10... Step: 3830... Loss: 3.2894... Val Loss: 3.2781\n",
      "Epoch: 9/10... Step: 3840... Loss: 3.3013... Val Loss: 3.2765\n",
      "Epoch: 9/10... Step: 3850... Loss: 3.2888... Val Loss: 3.2760\n",
      "Epoch: 9/10... Step: 3860... Loss: 3.2836... Val Loss: 3.2776\n",
      "Epoch: 9/10... Step: 3870... Loss: 3.2802... Val Loss: 3.2790\n",
      "Epoch: 9/10... Step: 3880... Loss: 3.2845... Val Loss: 3.2780\n",
      "Epoch: 9/10... Step: 3890... Loss: 3.3002... Val Loss: 3.2721\n",
      "Epoch: 9/10... Step: 3900... Loss: 3.2965... Val Loss: 3.2774\n",
      "Epoch: 9/10... Step: 3910... Loss: 3.2969... Val Loss: 3.2719\n",
      "Epoch: 9/10... Step: 3920... Loss: 3.3056... Val Loss: 3.2792\n",
      "Epoch: 9/10... Step: 3930... Loss: 3.3018... Val Loss: 3.2780\n",
      "Epoch: 9/10... Step: 3940... Loss: 3.3120... Val Loss: 3.2750\n",
      "Epoch: 9/10... Step: 3950... Loss: 3.2883... Val Loss: 3.2793\n",
      "Epoch: 9/10... Step: 3960... Loss: 3.3021... Val Loss: 3.2750\n",
      "Epoch: 9/10... Step: 3970... Loss: 3.2969... Val Loss: 3.2757\n",
      "Epoch: 9/10... Step: 3980... Loss: 3.2981... Val Loss: 3.2777\n",
      "Epoch: 9/10... Step: 3990... Loss: 3.2919... Val Loss: 3.2660\n",
      "Epoch: 9/10... Step: 4000... Loss: 3.3040... Val Loss: 3.2701\n",
      "Epoch: 9/10... Step: 4010... Loss: 3.2944... Val Loss: 3.2726\n",
      "Epoch: 9/10... Step: 4020... Loss: 3.3059... Val Loss: 3.2688\n",
      "Epoch: 9/10... Step: 4030... Loss: 3.2875... Val Loss: 3.2678\n",
      "Epoch: 9/10... Step: 4040... Loss: 3.2910... Val Loss: 3.2683\n",
      "Epoch: 9/10... Step: 4050... Loss: 3.2856... Val Loss: 3.2657\n",
      "Epoch: 10/10... Step: 4060... Loss: 3.2846... Val Loss: 3.2722\n",
      "Epoch: 10/10... Step: 4070... Loss: 3.3014... Val Loss: 3.2671\n",
      "Epoch: 10/10... Step: 4080... Loss: 3.3026... Val Loss: 3.2726\n",
      "Epoch: 10/10... Step: 4090... Loss: 3.3085... Val Loss: 3.2693\n",
      "Epoch: 10/10... Step: 4100... Loss: 3.3023... Val Loss: 3.2664\n",
      "Epoch: 10/10... Step: 4110... Loss: 3.2923... Val Loss: 3.2688\n",
      "Epoch: 10/10... Step: 4120... Loss: 3.3067... Val Loss: 3.2688\n",
      "Epoch: 10/10... Step: 4130... Loss: 3.3066... Val Loss: 3.2714\n",
      "Epoch: 10/10... Step: 4140... Loss: 3.2998... Val Loss: 3.2713\n",
      "Epoch: 10/10... Step: 4150... Loss: 3.3089... Val Loss: 3.2741\n",
      "Epoch: 10/10... Step: 4160... Loss: 3.3117... Val Loss: 3.2757\n",
      "Epoch: 10/10... Step: 4170... Loss: 3.2991... Val Loss: 3.2724\n",
      "Epoch: 10/10... Step: 4180... Loss: 3.2954... Val Loss: 3.2715\n",
      "Epoch: 10/10... Step: 4190... Loss: 3.2953... Val Loss: 3.2746\n",
      "Epoch: 10/10... Step: 4200... Loss: 3.2910... Val Loss: 3.2756\n",
      "Epoch: 10/10... Step: 4210... Loss: 3.2932... Val Loss: 3.2751\n",
      "Epoch: 10/10... Step: 4220... Loss: 3.3054... Val Loss: 3.2721\n",
      "Epoch: 10/10... Step: 4230... Loss: 3.2905... Val Loss: 3.2778\n",
      "Epoch: 10/10... Step: 4240... Loss: 3.3154... Val Loss: 3.2735\n",
      "Epoch: 10/10... Step: 4250... Loss: 3.3019... Val Loss: 3.2761\n",
      "Epoch: 10/10... Step: 4260... Loss: 3.3206... Val Loss: 3.2741\n",
      "Epoch: 10/10... Step: 4270... Loss: 3.3067... Val Loss: 3.2705\n",
      "Epoch: 10/10... Step: 4280... Loss: 3.2895... Val Loss: 3.2780\n",
      "Epoch: 10/10... Step: 4290... Loss: 3.3018... Val Loss: 3.2764\n",
      "Epoch: 10/10... Step: 4300... Loss: 3.2887... Val Loss: 3.2762\n",
      "Epoch: 10/10... Step: 4310... Loss: 3.2841... Val Loss: 3.2774\n",
      "Epoch: 10/10... Step: 4320... Loss: 3.2796... Val Loss: 3.2790\n",
      "Epoch: 10/10... Step: 4330... Loss: 3.2846... Val Loss: 3.2782\n",
      "Epoch: 10/10... Step: 4340... Loss: 3.3005... Val Loss: 3.2721\n",
      "Epoch: 10/10... Step: 4350... Loss: 3.2965... Val Loss: 3.2774\n",
      "Epoch: 10/10... Step: 4360... Loss: 3.2968... Val Loss: 3.2722\n",
      "Epoch: 10/10... Step: 4370... Loss: 3.3050... Val Loss: 3.2787\n",
      "Epoch: 10/10... Step: 4380... Loss: 3.3026... Val Loss: 3.2783\n",
      "Epoch: 10/10... Step: 4390... Loss: 3.3115... Val Loss: 3.2749\n",
      "Epoch: 10/10... Step: 4400... Loss: 3.2885... Val Loss: 3.2794\n",
      "Epoch: 10/10... Step: 4410... Loss: 3.3023... Val Loss: 3.2753\n",
      "Epoch: 10/10... Step: 4420... Loss: 3.2971... Val Loss: 3.2756\n",
      "Epoch: 10/10... Step: 4430... Loss: 3.2981... Val Loss: 3.2779\n",
      "Epoch: 10/10... Step: 4440... Loss: 3.2921... Val Loss: 3.2662\n",
      "Epoch: 10/10... Step: 4450... Loss: 3.3042... Val Loss: 3.2699\n",
      "Epoch: 10/10... Step: 4460... Loss: 3.2948... Val Loss: 3.2728\n",
      "Epoch: 10/10... Step: 4470... Loss: 3.3059... Val Loss: 3.2688\n",
      "Epoch: 10/10... Step: 4480... Loss: 3.2879... Val Loss: 3.2678\n",
      "Epoch: 10/10... Step: 4490... Loss: 3.2914... Val Loss: 3.2680\n",
      "Epoch: 10/10... Step: 4500... Loss: 3.2853... Val Loss: 3.2661\n",
      "Validation loss decreased (inf --> 3.260407).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 10 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:39:13.962461Z",
     "iopub.status.busy": "2024-01-23T21:39:13.962097Z",
     "iopub.status.idle": "2024-01-23T21:39:14.813304Z",
     "shell.execute_reply": "2024-01-23T21:39:14.812498Z",
     "shell.execute_reply.started": "2024-01-23T21:39:13.962434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/kaggle/input/temprnn/Temp.net', 'rb') as f:\n",
    "    if train_on_gpu:\n",
    "            checkpoint = torch.load(f)\n",
    "    else:        \n",
    "        checkpoint = torch.load(f,map_location=torch.device('cpu'))\n",
    "\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:39:14.814884Z",
     "iopub.status.busy": "2024-01-23T21:39:14.814553Z",
     "iopub.status.idle": "2024-01-23T21:39:14.823882Z",
     "shell.execute_reply": "2024-01-23T21:39:14.823043Z",
     "shell.execute_reply.started": "2024-01-23T21:39:14.814853Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:39:14.825176Z",
     "iopub.status.busy": "2024-01-23T21:39:14.824902Z",
     "iopub.status.idle": "2024-01-23T21:39:14.838067Z",
     "shell.execute_reply": "2024-01-23T21:39:14.837364Z",
     "shell.execute_reply.started": "2024-01-23T21:39:14.825135Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:39:14.839216Z",
     "iopub.status.busy": "2024-01-23T21:39:14.838971Z",
     "iopub.status.idle": "2024-01-23T21:39:15.079195Z",
     "shell.execute_reply": "2024-01-23T21:39:15.078342Z",
     "shell.execute_reply.started": "2024-01-23T21:39:14.839195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 300, prime='', top_k=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4346011,
     "sourceId": 7466206,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4346021,
     "sourceId": 7466234,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
